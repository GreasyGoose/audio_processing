{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asr_lab_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RcyxmRJGqlY"
      },
      "source": [
        "# Практика №4\n",
        "\n",
        "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbO_rrWGq7j"
      },
      "source": [
        "### Bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzJyomV1JaLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f98182-968a-4702-bc59-fbe6592356ec"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (1.19.5)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROAsHTXHWik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62087c5c-72b6-4e4b-9c9d-744287aeee4d"
      },
      "source": [
        "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
        "\n",
        "!unzip -q lab4.zip\n",
        "!rm -rf lab4.zip sample_data\n",
        "%cd lab4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6\n",
            "To: /content/lab4.zip\n",
            "\r0.00B [00:00, ?B/s]\r2.77MB [00:00, 87.1MB/s]\n",
            "/content/lab4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4wcCtkIH2dn"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from utils import TextTransform\n",
        "from utils import cer\n",
        "from utils import wer\n",
        "\n",
        "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
        "from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
        "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
        "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
        "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaESUZiHJgfN"
      },
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000,\n",
        "                                                              n_fft=400,\n",
        "                                                              hop_length=160,\n",
        "                                                              n_mels=80)\n",
        "\n",
        "# text_transform = TextTransform() #for baseline Transformer model\n",
        "\n",
        "#-----------------------------TODO №2-----------------------------------\n",
        "# Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
        "text_transform = TextTransformBPE('/content/lab4/train_clean_100_text_clean.txt')\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=4000, collapse_repeated=True): #blank_label=28 for baseline Transformer model with TextTransform()\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqoVLnrJsCV"
      },
      "source": [
        "class TransformerModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, dropout\n",
        "                ),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2p_8IjeKkqq"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
        "        spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item(), scheduler.get_last_lr()[0]))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
        "            \n",
        "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "            input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzEbtsB1LKsh"
      },
      "source": [
        "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
        "    \n",
        "    hparams = {\n",
        "        \"input_size\": 80,\n",
        "        \"output_size\": 4001, #\"output_size\": 29 for baseline Transformer model with TextTransform() \n",
        "        \"conv2d_filters\": 32,\n",
        "        \"attention_dim\": 360,\n",
        "        \"attention_heads\": 8,\n",
        "        \"feedforward_dim\": 1024,\n",
        "        \"num_layers\":10,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=test_batch_size,\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = TransformerModel(\n",
        "        hparams['input_size'],\n",
        "        hparams['output_size'],\n",
        "        hparams['conv2d_filters'],\n",
        "        hparams['attention_dim'],\n",
        "        hparams['attention_heads'],\n",
        "        hparams['feedforward_dim'],\n",
        "        hparams['num_layers'],\n",
        "        hparams['dropout']).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = torch.nn.CTCLoss(blank=4000, zero_infinity=False).to(device) # blank=28 for init model with TextTransform()\n",
        "\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        !date\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "        test(model, device, test_loader, criterion, epoch)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExZLsUiLeXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f648b7c5-9f48-44bc-9bff-49b4e7740345"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 10\n",
        "test_batch_size = 7\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransformerModel(\n",
            "  (conv_in): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conv_out): Sequential(\n",
            "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
            "    (1): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_layer): MultiSequential(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (1): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (2): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (3): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (4): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (5): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (6): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (7): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (8): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (9): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "  (final_layer): Linear(in_features=360, out_features=4001, bias=True)\n",
            ")\n",
            "Num Model Parameters 14284849\n",
            "Fri May  7 16:33:09 UTC 2021\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 52.846657\tLR: 0.000040\n",
            "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 7.230588\tLR: 0.000051\n",
            "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 6.892086\tLR: 0.000063\n",
            "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 6.857765\tLR: 0.000074\n",
            "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 6.955415\tLR: 0.000085\n",
            "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 6.869297\tLR: 0.000096\n",
            "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 6.866670\tLR: 0.000107\n",
            "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 6.976621\tLR: 0.000119\n",
            "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 7.072049\tLR: 0.000130\n",
            "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 6.853961\tLR: 0.000141\n",
            "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 6.927845\tLR: 0.000152\n",
            "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 6.932575\tLR: 0.000163\n",
            "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 6.890894\tLR: 0.000175\n",
            "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 6.846017\tLR: 0.000186\n",
            "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 6.938826\tLR: 0.000197\n",
            "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 6.768041\tLR: 0.000208\n",
            "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 6.868493\tLR: 0.000220\n",
            "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 6.839544\tLR: 0.000231\n",
            "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 6.979089\tLR: 0.000242\n",
            "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 7.035347\tLR: 0.000253\n",
            "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 6.754252\tLR: 0.000264\n",
            "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 6.783969\tLR: 0.000276\n",
            "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 6.946663\tLR: 0.000287\n",
            "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 6.597596\tLR: 0.000298\n",
            "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 6.633028\tLR: 0.000309\n",
            "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 6.641631\tLR: 0.000320\n",
            "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 6.760459\tLR: 0.000332\n",
            "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 6.709554\tLR: 0.000343\n",
            "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 6.682199\tLR: 0.000354\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 6.6404, Average CER: 0.997403 Average WER: 0.9983\n",
            "\n",
            "Fri May  7 17:00:58 UTC 2021\n",
            "Train Epoch: 2 [0/28539 (0%)]\tLoss: 6.593834\tLR: 0.000360\n",
            "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 6.647081\tLR: 0.000371\n",
            "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 6.610691\tLR: 0.000383\n",
            "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 6.317815\tLR: 0.000394\n",
            "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 6.242479\tLR: 0.000405\n",
            "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 6.305305\tLR: 0.000416\n",
            "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 6.432704\tLR: 0.000427\n",
            "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 6.359050\tLR: 0.000439\n",
            "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 6.456099\tLR: 0.000450\n",
            "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 5.605296\tLR: 0.000461\n",
            "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 6.250254\tLR: 0.000472\n",
            "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 6.062363\tLR: 0.000483\n",
            "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 6.074932\tLR: 0.000495\n",
            "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 6.041218\tLR: 0.000506\n",
            "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 5.776670\tLR: 0.000517\n",
            "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 5.883777\tLR: 0.000528\n",
            "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 5.573515\tLR: 0.000540\n",
            "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 5.522268\tLR: 0.000551\n",
            "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 5.819127\tLR: 0.000562\n",
            "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 5.515669\tLR: 0.000573\n",
            "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 5.699695\tLR: 0.000584\n",
            "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 5.611022\tLR: 0.000596\n",
            "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 5.305463\tLR: 0.000607\n",
            "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 5.215686\tLR: 0.000618\n",
            "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 5.183969\tLR: 0.000629\n",
            "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 4.886507\tLR: 0.000640\n",
            "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 5.434007\tLR: 0.000652\n",
            "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 5.380107\tLR: 0.000663\n",
            "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 5.143382\tLR: 0.000674\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.8985, Average CER: 0.770797 Average WER: 0.8705\n",
            "\n",
            "Fri May  7 17:30:46 UTC 2021\n",
            "Train Epoch: 3 [0/28539 (0%)]\tLoss: 5.107123\tLR: 0.000680\n",
            "Train Epoch: 3 [1000/28539 (4%)]\tLoss: 5.379844\tLR: 0.000691\n",
            "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 5.013255\tLR: 0.000703\n",
            "Train Epoch: 3 [3000/28539 (11%)]\tLoss: 5.366868\tLR: 0.000714\n",
            "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 5.266306\tLR: 0.000725\n",
            "Train Epoch: 3 [5000/28539 (18%)]\tLoss: 4.956647\tLR: 0.000736\n",
            "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 5.118694\tLR: 0.000747\n",
            "Train Epoch: 3 [7000/28539 (25%)]\tLoss: 4.630965\tLR: 0.000759\n",
            "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 5.041187\tLR: 0.000770\n",
            "Train Epoch: 3 [9000/28539 (32%)]\tLoss: 5.080768\tLR: 0.000781\n",
            "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 4.930605\tLR: 0.000792\n",
            "Train Epoch: 3 [11000/28539 (39%)]\tLoss: 5.170529\tLR: 0.000804\n",
            "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 4.857555\tLR: 0.000815\n",
            "Train Epoch: 3 [13000/28539 (46%)]\tLoss: 4.766452\tLR: 0.000826\n",
            "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 4.900503\tLR: 0.000837\n",
            "Train Epoch: 3 [15000/28539 (53%)]\tLoss: 5.139542\tLR: 0.000848\n",
            "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 5.053559\tLR: 0.000860\n",
            "Train Epoch: 3 [17000/28539 (60%)]\tLoss: 4.403868\tLR: 0.000871\n",
            "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 4.828250\tLR: 0.000882\n",
            "Train Epoch: 3 [19000/28539 (67%)]\tLoss: 4.934130\tLR: 0.000893\n",
            "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 4.768476\tLR: 0.000904\n",
            "Train Epoch: 3 [21000/28539 (74%)]\tLoss: 4.631168\tLR: 0.000916\n",
            "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 4.705909\tLR: 0.000927\n",
            "Train Epoch: 3 [23000/28539 (81%)]\tLoss: 4.737338\tLR: 0.000938\n",
            "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 4.786904\tLR: 0.000949\n",
            "Train Epoch: 3 [25000/28539 (88%)]\tLoss: 4.721827\tLR: 0.000961\n",
            "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 4.593549\tLR: 0.000972\n",
            "Train Epoch: 3 [27000/28539 (95%)]\tLoss: 4.969579\tLR: 0.000983\n",
            "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 4.730032\tLR: 0.000994\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.3175, Average CER: 0.652336 Average WER: 0.7987\n",
            "\n",
            "Fri May  7 18:01:38 UTC 2021\n",
            "Train Epoch: 4 [0/28539 (0%)]\tLoss: 4.573146\tLR: 0.001000\n",
            "Train Epoch: 4 [1000/28539 (4%)]\tLoss: 4.856130\tLR: 0.000995\n",
            "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 4.557167\tLR: 0.000990\n",
            "Train Epoch: 4 [3000/28539 (11%)]\tLoss: 4.660342\tLR: 0.000985\n",
            "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 4.585494\tLR: 0.000980\n",
            "Train Epoch: 4 [5000/28539 (18%)]\tLoss: 4.597587\tLR: 0.000975\n",
            "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 4.549626\tLR: 0.000970\n",
            "Train Epoch: 4 [7000/28539 (25%)]\tLoss: 4.782145\tLR: 0.000965\n",
            "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 4.088492\tLR: 0.000960\n",
            "Train Epoch: 4 [9000/28539 (32%)]\tLoss: 4.809762\tLR: 0.000955\n",
            "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 4.710787\tLR: 0.000950\n",
            "Train Epoch: 4 [11000/28539 (39%)]\tLoss: 4.737038\tLR: 0.000945\n",
            "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 4.492644\tLR: 0.000940\n",
            "Train Epoch: 4 [13000/28539 (46%)]\tLoss: 4.496408\tLR: 0.000935\n",
            "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 4.920897\tLR: 0.000930\n",
            "Train Epoch: 4 [15000/28539 (53%)]\tLoss: 4.774986\tLR: 0.000925\n",
            "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 4.462708\tLR: 0.000920\n",
            "Train Epoch: 4 [17000/28539 (60%)]\tLoss: 4.260801\tLR: 0.000915\n",
            "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 3.938712\tLR: 0.000910\n",
            "Train Epoch: 4 [19000/28539 (67%)]\tLoss: 4.614501\tLR: 0.000905\n",
            "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 4.387046\tLR: 0.000900\n",
            "Train Epoch: 4 [21000/28539 (74%)]\tLoss: 4.518025\tLR: 0.000895\n",
            "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 4.325643\tLR: 0.000890\n",
            "Train Epoch: 4 [23000/28539 (81%)]\tLoss: 4.089587\tLR: 0.000885\n",
            "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 4.071931\tLR: 0.000880\n",
            "Train Epoch: 4 [25000/28539 (88%)]\tLoss: 4.489387\tLR: 0.000875\n",
            "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 4.314374\tLR: 0.000870\n",
            "Train Epoch: 4 [27000/28539 (95%)]\tLoss: 4.127888\tLR: 0.000865\n",
            "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 4.062657\tLR: 0.000860\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 3.6522, Average CER: 0.546940 Average WER: 0.7254\n",
            "\n",
            "Fri May  7 18:33:22 UTC 2021\n",
            "Train Epoch: 5 [0/28539 (0%)]\tLoss: 3.885553\tLR: 0.000857\n",
            "Train Epoch: 5 [1000/28539 (4%)]\tLoss: 4.219637\tLR: 0.000852\n",
            "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 3.900212\tLR: 0.000847\n",
            "Train Epoch: 5 [3000/28539 (11%)]\tLoss: 3.896195\tLR: 0.000842\n",
            "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 3.841201\tLR: 0.000837\n",
            "Train Epoch: 5 [5000/28539 (18%)]\tLoss: 3.655263\tLR: 0.000832\n",
            "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 3.895101\tLR: 0.000827\n",
            "Train Epoch: 5 [7000/28539 (25%)]\tLoss: 3.670272\tLR: 0.000822\n",
            "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 3.839139\tLR: 0.000817\n",
            "Train Epoch: 5 [9000/28539 (32%)]\tLoss: 3.926299\tLR: 0.000812\n",
            "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 4.023908\tLR: 0.000807\n",
            "Train Epoch: 5 [11000/28539 (39%)]\tLoss: 3.711636\tLR: 0.000802\n",
            "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 3.382877\tLR: 0.000797\n",
            "Train Epoch: 5 [13000/28539 (46%)]\tLoss: 3.957544\tLR: 0.000792\n",
            "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 3.684108\tLR: 0.000787\n",
            "Train Epoch: 5 [15000/28539 (53%)]\tLoss: 3.780306\tLR: 0.000782\n",
            "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 3.424549\tLR: 0.000777\n",
            "Train Epoch: 5 [17000/28539 (60%)]\tLoss: 3.491763\tLR: 0.000772\n",
            "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 3.538340\tLR: 0.000767\n",
            "Train Epoch: 5 [19000/28539 (67%)]\tLoss: 4.062531\tLR: 0.000762\n",
            "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 3.498383\tLR: 0.000757\n",
            "Train Epoch: 5 [21000/28539 (74%)]\tLoss: 4.088971\tLR: 0.000752\n",
            "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 3.599157\tLR: 0.000747\n",
            "Train Epoch: 5 [23000/28539 (81%)]\tLoss: 3.622272\tLR: 0.000742\n",
            "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 3.352601\tLR: 0.000737\n",
            "Train Epoch: 5 [25000/28539 (88%)]\tLoss: 3.638136\tLR: 0.000732\n",
            "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 3.225410\tLR: 0.000727\n",
            "Train Epoch: 5 [27000/28539 (95%)]\tLoss: 3.550559\tLR: 0.000722\n",
            "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 3.267270\tLR: 0.000717\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 3.0327, Average CER: 0.438167 Average WER: 0.6348\n",
            "\n",
            "Fri May  7 19:06:05 UTC 2021\n",
            "Train Epoch: 6 [0/28539 (0%)]\tLoss: 3.126015\tLR: 0.000714\n",
            "Train Epoch: 6 [1000/28539 (4%)]\tLoss: 3.243598\tLR: 0.000709\n",
            "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 3.429710\tLR: 0.000704\n",
            "Train Epoch: 6 [3000/28539 (11%)]\tLoss: 3.593684\tLR: 0.000699\n",
            "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 3.403063\tLR: 0.000694\n",
            "Train Epoch: 6 [5000/28539 (18%)]\tLoss: 3.467332\tLR: 0.000689\n",
            "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 3.352501\tLR: 0.000684\n",
            "Train Epoch: 6 [7000/28539 (25%)]\tLoss: 3.521597\tLR: 0.000679\n",
            "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 3.251961\tLR: 0.000674\n",
            "Train Epoch: 6 [9000/28539 (32%)]\tLoss: 3.253094\tLR: 0.000669\n",
            "Train Epoch: 6 [10000/28539 (35%)]\tLoss: 3.556717\tLR: 0.000664\n",
            "Train Epoch: 6 [11000/28539 (39%)]\tLoss: 3.256165\tLR: 0.000659\n",
            "Train Epoch: 6 [12000/28539 (42%)]\tLoss: 3.589794\tLR: 0.000654\n",
            "Train Epoch: 6 [13000/28539 (46%)]\tLoss: 3.291356\tLR: 0.000649\n",
            "Train Epoch: 6 [14000/28539 (49%)]\tLoss: 3.748237\tLR: 0.000644\n",
            "Train Epoch: 6 [15000/28539 (53%)]\tLoss: 3.332036\tLR: 0.000639\n",
            "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 3.434245\tLR: 0.000634\n",
            "Train Epoch: 6 [17000/28539 (60%)]\tLoss: 3.943915\tLR: 0.000629\n",
            "Train Epoch: 6 [18000/28539 (63%)]\tLoss: 3.041485\tLR: 0.000624\n",
            "Train Epoch: 6 [19000/28539 (67%)]\tLoss: 3.223222\tLR: 0.000619\n",
            "Train Epoch: 6 [20000/28539 (70%)]\tLoss: 3.675666\tLR: 0.000614\n",
            "Train Epoch: 6 [21000/28539 (74%)]\tLoss: 3.157449\tLR: 0.000609\n",
            "Train Epoch: 6 [22000/28539 (77%)]\tLoss: 2.992994\tLR: 0.000604\n",
            "Train Epoch: 6 [23000/28539 (81%)]\tLoss: 3.576928\tLR: 0.000599\n",
            "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 3.093088\tLR: 0.000594\n",
            "Train Epoch: 6 [25000/28539 (88%)]\tLoss: 3.051927\tLR: 0.000589\n",
            "Train Epoch: 6 [26000/28539 (91%)]\tLoss: 3.291239\tLR: 0.000584\n",
            "Train Epoch: 6 [27000/28539 (95%)]\tLoss: 3.247154\tLR: 0.000579\n",
            "Train Epoch: 6 [28000/28539 (98%)]\tLoss: 3.068161\tLR: 0.000574\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.6862, Average CER: 0.393001 Average WER: 0.5890\n",
            "\n",
            "Fri May  7 19:39:00 UTC 2021\n",
            "Train Epoch: 7 [0/28539 (0%)]\tLoss: 3.312881\tLR: 0.000571\n",
            "Train Epoch: 7 [1000/28539 (4%)]\tLoss: 2.808973\tLR: 0.000566\n",
            "Train Epoch: 7 [2000/28539 (7%)]\tLoss: 3.082886\tLR: 0.000561\n",
            "Train Epoch: 7 [3000/28539 (11%)]\tLoss: 3.183266\tLR: 0.000556\n",
            "Train Epoch: 7 [4000/28539 (14%)]\tLoss: 2.991055\tLR: 0.000551\n",
            "Train Epoch: 7 [5000/28539 (18%)]\tLoss: 2.959437\tLR: 0.000546\n",
            "Train Epoch: 7 [6000/28539 (21%)]\tLoss: 2.975089\tLR: 0.000541\n",
            "Train Epoch: 7 [7000/28539 (25%)]\tLoss: 2.890030\tLR: 0.000536\n",
            "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 3.238029\tLR: 0.000531\n",
            "Train Epoch: 7 [9000/28539 (32%)]\tLoss: 3.173906\tLR: 0.000526\n",
            "Train Epoch: 7 [10000/28539 (35%)]\tLoss: 2.659022\tLR: 0.000521\n",
            "Train Epoch: 7 [11000/28539 (39%)]\tLoss: 3.061665\tLR: 0.000516\n",
            "Train Epoch: 7 [12000/28539 (42%)]\tLoss: 3.270573\tLR: 0.000511\n",
            "Train Epoch: 7 [13000/28539 (46%)]\tLoss: 2.880644\tLR: 0.000506\n",
            "Train Epoch: 7 [14000/28539 (49%)]\tLoss: 2.963540\tLR: 0.000501\n",
            "Train Epoch: 7 [15000/28539 (53%)]\tLoss: 3.272981\tLR: 0.000496\n",
            "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 2.832994\tLR: 0.000491\n",
            "Train Epoch: 7 [17000/28539 (60%)]\tLoss: 2.786907\tLR: 0.000486\n",
            "Train Epoch: 7 [18000/28539 (63%)]\tLoss: 2.724622\tLR: 0.000481\n",
            "Train Epoch: 7 [19000/28539 (67%)]\tLoss: 3.445220\tLR: 0.000476\n",
            "Train Epoch: 7 [20000/28539 (70%)]\tLoss: 2.949646\tLR: 0.000471\n",
            "Train Epoch: 7 [21000/28539 (74%)]\tLoss: 2.691478\tLR: 0.000466\n",
            "Train Epoch: 7 [22000/28539 (77%)]\tLoss: 3.223236\tLR: 0.000461\n",
            "Train Epoch: 7 [23000/28539 (81%)]\tLoss: 2.568427\tLR: 0.000456\n",
            "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 3.030752\tLR: 0.000451\n",
            "Train Epoch: 7 [25000/28539 (88%)]\tLoss: 2.783565\tLR: 0.000446\n",
            "Train Epoch: 7 [26000/28539 (91%)]\tLoss: 3.172496\tLR: 0.000441\n",
            "Train Epoch: 7 [27000/28539 (95%)]\tLoss: 2.941289\tLR: 0.000436\n",
            "Train Epoch: 7 [28000/28539 (98%)]\tLoss: 2.766844\tLR: 0.000431\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.4199, Average CER: 0.351861 Average WER: 0.5438\n",
            "\n",
            "Fri May  7 20:11:36 UTC 2021\n",
            "Train Epoch: 8 [0/28539 (0%)]\tLoss: 2.856547\tLR: 0.000428\n",
            "Train Epoch: 8 [1000/28539 (4%)]\tLoss: 2.749166\tLR: 0.000423\n",
            "Train Epoch: 8 [2000/28539 (7%)]\tLoss: 3.296186\tLR: 0.000418\n",
            "Train Epoch: 8 [3000/28539 (11%)]\tLoss: 2.791431\tLR: 0.000413\n",
            "Train Epoch: 8 [4000/28539 (14%)]\tLoss: 2.660963\tLR: 0.000408\n",
            "Train Epoch: 8 [5000/28539 (18%)]\tLoss: 2.740352\tLR: 0.000403\n",
            "Train Epoch: 8 [6000/28539 (21%)]\tLoss: 2.636131\tLR: 0.000398\n",
            "Train Epoch: 8 [7000/28539 (25%)]\tLoss: 2.926552\tLR: 0.000393\n",
            "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 2.826569\tLR: 0.000388\n",
            "Train Epoch: 8 [9000/28539 (32%)]\tLoss: 2.658386\tLR: 0.000383\n",
            "Train Epoch: 8 [10000/28539 (35%)]\tLoss: 2.597140\tLR: 0.000378\n",
            "Train Epoch: 8 [11000/28539 (39%)]\tLoss: 2.922909\tLR: 0.000373\n",
            "Train Epoch: 8 [12000/28539 (42%)]\tLoss: 2.306646\tLR: 0.000368\n",
            "Train Epoch: 8 [13000/28539 (46%)]\tLoss: 3.007289\tLR: 0.000363\n",
            "Train Epoch: 8 [14000/28539 (49%)]\tLoss: 2.761044\tLR: 0.000358\n",
            "Train Epoch: 8 [15000/28539 (53%)]\tLoss: 2.712910\tLR: 0.000353\n",
            "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 2.989808\tLR: 0.000348\n",
            "Train Epoch: 8 [17000/28539 (60%)]\tLoss: 2.677059\tLR: 0.000343\n",
            "Train Epoch: 8 [18000/28539 (63%)]\tLoss: 2.834436\tLR: 0.000338\n",
            "Train Epoch: 8 [19000/28539 (67%)]\tLoss: 2.616490\tLR: 0.000333\n",
            "Train Epoch: 8 [20000/28539 (70%)]\tLoss: 2.814953\tLR: 0.000328\n",
            "Train Epoch: 8 [21000/28539 (74%)]\tLoss: 2.478189\tLR: 0.000323\n",
            "Train Epoch: 8 [22000/28539 (77%)]\tLoss: 2.678934\tLR: 0.000318\n",
            "Train Epoch: 8 [23000/28539 (81%)]\tLoss: 2.584332\tLR: 0.000313\n",
            "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 2.685735\tLR: 0.000308\n",
            "Train Epoch: 8 [25000/28539 (88%)]\tLoss: 2.796969\tLR: 0.000303\n",
            "Train Epoch: 8 [26000/28539 (91%)]\tLoss: 2.720129\tLR: 0.000298\n",
            "Train Epoch: 8 [27000/28539 (95%)]\tLoss: 2.615068\tLR: 0.000293\n",
            "Train Epoch: 8 [28000/28539 (98%)]\tLoss: 3.035856\tLR: 0.000288\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.2833, Average CER: 0.319371 Average WER: 0.5243\n",
            "\n",
            "Fri May  7 20:44:32 UTC 2021\n",
            "Train Epoch: 9 [0/28539 (0%)]\tLoss: 2.415925\tLR: 0.000286\n",
            "Train Epoch: 9 [1000/28539 (4%)]\tLoss: 2.798985\tLR: 0.000281\n",
            "Train Epoch: 9 [2000/28539 (7%)]\tLoss: 2.727382\tLR: 0.000276\n",
            "Train Epoch: 9 [3000/28539 (11%)]\tLoss: 2.485217\tLR: 0.000271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mby39YVqZadd"
      },
      "source": [
        "### <b>Задание №1</b> (5 баллов):\n",
        "На данный момент практически все E2E SOTA решения использую [сабворды](https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/) (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Главное правильно обернуть его в наш класс TextTransform. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOtjm4WNbutB",
        "outputId": "0a00e1f7-e9f1-458d-ce04-a783dd65983b"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 15.7MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 9.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 10.9MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 11.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 11.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 10.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 10.8MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 10.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 10.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 10.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 10.8MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 10.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 10.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 10.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McBm5u8q__hy"
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbiW919e2le"
      },
      "source": [
        "class TextTransformBPE:\n",
        "    def __init__(self, train_text):\n",
        "        \"\"\" Обучение BPE модели на 4000 юнитов\"\"\"\n",
        "        spm.SentencePieceTrainer.train(input=train_text,\n",
        "                                       model_prefix='m', \n",
        "                                       vocab_size=4000,\n",
        "                                       model_type = 'bpe')\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load('m.model')\n",
        "        # pass\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Преобразование входного текста в последовательность сабвордов в формате их индекса в BPE модели \"\"\"\n",
        "        # int_sequence = []\n",
        "        # pass\n",
        "        int_sequence = self.sp.encode_as_ids(text.upper())#.upper()\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Преобразование последовательности индексов сабвордов в текст \"\"\"\n",
        "        # string = []\n",
        "        # pass\n",
        "        string = self.sp.decode_ids([int(label) for label in labels])\n",
        "        return string"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaCRvuL9SmzY"
      },
      "source": [
        "transform_test = TextTransformBPE('/content/lab4/train_clean_100_text_clean.txt')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDaJXZbNS7oQ",
        "outputId": "d9e35064-28f8-40de-8884-617584970024"
      },
      "source": [
        "test_string = 'TEST_Sentence zero NY'\n",
        "transform_test.text_to_int(test_string.lower())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3654, 0, 3981, 66, 300, 3973, 0, 15, 3977, 43, 3991]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib6GlwqWb-H-",
        "outputId": "4c9e545b-aa8f-402a-d6c5-df2db584b963"
      },
      "source": [
        "transform_test.text_to_int(r\"This_is-a's test\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3973, 0, 3996, 0, 3973, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8RpRjR-fldKI",
        "outputId": "474ed619-d13b-42b5-dfc1-af61ca166c3a"
      },
      "source": [
        "transform_test.int_to_text([141, 0, 28, 0, 3976, 3996, 3981, 3654])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"THIS ⁇ IS ⁇ A'S TEST\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NL-lS_SuoIVO",
        "outputId": "6d97749d-d6a8-4a05-e985-74653a3dad6b"
      },
      "source": [
        "transform_test.text_to_int(' ')\n",
        "transform_test.text_to_int(' Z ')\n",
        "# transform_test.int_to_text([3973])\n",
        "# transform_test.int_to_text([0])\n",
        "# transform_test.int_to_text([3973,0])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' ⁇ '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IvLrY_QEovn9",
        "outputId": "eb47575c-67f3-412e-c257-d176276ada06"
      },
      "source": [
        "transform_test.int_to_text([209, 31, 9, 375, 586])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WHAT TH SANG REC'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7jxao4UHyIh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Для baseline модели с TextTransform() обучение на 10 эпохах дало WER порядка 0.5 <br>\n",
        "Для модели с TextTransformBPE аналогичный результат был достигнут к 9 эпохе. Можно сказать, по WER результат модели с сабвордовым токенайзером примерно на эпоху обгонял простой TextTransform().\n",
        "\n",
        "Логи обучения в resources:\n",
        "\n",
        "\n",
        "*   baseline_train_log.txt (by Dmitry Klestov)\n",
        "*   bpe_train_log.txt (my model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV48Q7HqZsAD"
      },
      "source": [
        "### <b>Задание №2</b> (5 баллов):\n",
        "Импровизация по улучшению качества распознавания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUeNm_q5kBSP"
      },
      "source": [
        "from espnet.nets.pytorch_backend.conformer.encoder_layer import EncoderLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3H-W_YcYVz"
      },
      "source": [
        "class ConformerModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        # positionwise_layer = PositionwiseFeedForward\n",
        "        # positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        # self.encoder_layer = repeat(\n",
        "        #     num_layers,\n",
        "        #     lambda lnum: EncoderLayer(\n",
        "        #         attention_dim,\n",
        "        #         MultiHeadedAttention(\n",
        "        #             attention_heads, attention_dim, dropout\n",
        "        #         ),\n",
        "        #         positionwise_layer(*positionwise_layer_args),\n",
        "        #         dropout,\n",
        "        #         normalize_before=True,\n",
        "        #         concat_after=False,\n",
        "        #     ),\n",
        "        # )\n",
        "        self.encoders = repeat(\n",
        "            num_blocks,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                encoder_selfattn_layer(*encoder_selfattn_layer_args),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                positionwise_layer(*positionwise_layer_args) if macaron_style else None,\n",
        "                convolution_layer(*convolution_layer_args) if use_cnn_module else None,\n",
        "                dropout_rate,\n",
        "                normalize_before,\n",
        "                concat_after,\n",
        "            ),\n",
        "            use_checkpointing,\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sepSCRkajeGz"
      },
      "source": [
        "NOT IMPLEMENTED YET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1uZIra6iGL2"
      },
      "source": [
        "# TODO: test training with conform model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}