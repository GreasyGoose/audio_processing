{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asr_lab_4_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RcyxmRJGqlY"
      },
      "source": [
        "# Практика №4\n",
        "\n",
        "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbO_rrWGq7j"
      },
      "source": [
        "### Bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzJyomV1JaLp"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROAsHTXHWik"
      },
      "source": [
        "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
        "\n",
        "!unzip -q lab4.zip\n",
        "!rm -rf lab4.zip sample_data\n",
        "%cd lab4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4wcCtkIH2dn"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from utils import TextTransform\n",
        "from utils import cer\n",
        "from utils import wer\n",
        "\n",
        "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
        "from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
        "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
        "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
        "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaESUZiHJgfN"
      },
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000,\n",
        "                                                              n_fft=400,\n",
        "                                                              hop_length=160,\n",
        "                                                              n_mels=80)\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "#-----------------------------TODO №2-----------------------------------\n",
        "# Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqoVLnrJsCV"
      },
      "source": [
        "class TransformerModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, dropout\n",
        "                ),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2p_8IjeKkqq"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
        "        spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item(), scheduler.get_last_lr()[0]))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
        "            \n",
        "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "            input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzEbtsB1LKsh"
      },
      "source": [
        "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
        "    \n",
        "    hparams = {\n",
        "        \"input_size\": 80,\n",
        "        \"output_size\": 29,\n",
        "        \"conv2d_filters\": 32,\n",
        "        \"attention_dim\": 360,\n",
        "        \"attention_heads\": 8,\n",
        "        \"feedforward_dim\": 1024,\n",
        "        \"num_layers\":10,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=test_batch_size,\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = TransformerModel(\n",
        "        hparams['input_size'],\n",
        "        hparams['output_size'],\n",
        "        hparams['conv2d_filters'],\n",
        "        hparams['attention_dim'],\n",
        "        hparams['attention_heads'],\n",
        "        hparams['feedforward_dim'],\n",
        "        hparams['num_layers'],\n",
        "        hparams['dropout']).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = torch.nn.CTCLoss(blank=28, zero_infinity=False).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        !date\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "        test(model, device, test_loader, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExZLsUiLeXk"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 10\n",
        "test_batch_size = 7\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mby39YVqZadd"
      },
      "source": [
        "### <b>Задание №1</b> (5 баллов):\n",
        "На данный момент практически все E2E SOTA решения использую [сабворды](https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/) (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Главное правильно обернуть его в наш класс TextTransform. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbiW919e2le"
      },
      "source": [
        "class TextTransformBPE:\n",
        "    def __init__(self, train_text):\n",
        "        \"\"\" Обучение BPE модели на 4000 юнитов\"\"\"\n",
        "        pass\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Преобразование входного текста в последовательность сабвордов в формате их индекса в BPE модели \"\"\"\n",
        "        int_sequence = []\n",
        "        pass\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Преобразование последовательности индексов сабвордов в текст \"\"\"\n",
        "        string = []\n",
        "        pass\n",
        "        return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV48Q7HqZsAD"
      },
      "source": [
        "### <b>Задание №2</b> (5 баллов):\n",
        "Импровизация по улучшению качества распознавания."
      ]
    }
  ]
}