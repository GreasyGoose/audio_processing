TransformerModel(
  (conv_in): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
  )
  (conv_out): Sequential(
    (0): Linear(in_features=640, out_features=360, bias=True)
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (encoder_layer): MultiSequential(
    (0): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (1): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (2): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (3): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (4): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (5): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (6): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (7): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (8): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (9): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
  )
  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
  (final_layer): Linear(in_features=360, out_features=4001, bias=True)
)
Num Model Parameters 14284849
Fri May  7 16:33:09 UTC 2021
Train Epoch: 1 [0/28539 (0%)]	Loss: 52.846657	LR: 0.000040
Train Epoch: 1 [1000/28539 (4%)]	Loss: 7.230588	LR: 0.000051
Train Epoch: 1 [2000/28539 (7%)]	Loss: 6.892086	LR: 0.000063
Train Epoch: 1 [3000/28539 (11%)]	Loss: 6.857765	LR: 0.000074
Train Epoch: 1 [4000/28539 (14%)]	Loss: 6.955415	LR: 0.000085
Train Epoch: 1 [5000/28539 (18%)]	Loss: 6.869297	LR: 0.000096
Train Epoch: 1 [6000/28539 (21%)]	Loss: 6.866670	LR: 0.000107
Train Epoch: 1 [7000/28539 (25%)]	Loss: 6.976621	LR: 0.000119
Train Epoch: 1 [8000/28539 (28%)]	Loss: 7.072049	LR: 0.000130
Train Epoch: 1 [9000/28539 (32%)]	Loss: 6.853961	LR: 0.000141
Train Epoch: 1 [10000/28539 (35%)]	Loss: 6.927845	LR: 0.000152
Train Epoch: 1 [11000/28539 (39%)]	Loss: 6.932575	LR: 0.000163
Train Epoch: 1 [12000/28539 (42%)]	Loss: 6.890894	LR: 0.000175
Train Epoch: 1 [13000/28539 (46%)]	Loss: 6.846017	LR: 0.000186
Train Epoch: 1 [14000/28539 (49%)]	Loss: 6.938826	LR: 0.000197
Train Epoch: 1 [15000/28539 (53%)]	Loss: 6.768041	LR: 0.000208
Train Epoch: 1 [16000/28539 (56%)]	Loss: 6.868493	LR: 0.000220
Train Epoch: 1 [17000/28539 (60%)]	Loss: 6.839544	LR: 0.000231
Train Epoch: 1 [18000/28539 (63%)]	Loss: 6.979089	LR: 0.000242
Train Epoch: 1 [19000/28539 (67%)]	Loss: 7.035347	LR: 0.000253
Train Epoch: 1 [20000/28539 (70%)]	Loss: 6.754252	LR: 0.000264
Train Epoch: 1 [21000/28539 (74%)]	Loss: 6.783969	LR: 0.000276
Train Epoch: 1 [22000/28539 (77%)]	Loss: 6.946663	LR: 0.000287
Train Epoch: 1 [23000/28539 (81%)]	Loss: 6.597596	LR: 0.000298
Train Epoch: 1 [24000/28539 (84%)]	Loss: 6.633028	LR: 0.000309
Train Epoch: 1 [25000/28539 (88%)]	Loss: 6.641631	LR: 0.000320
Train Epoch: 1 [26000/28539 (91%)]	Loss: 6.760459	LR: 0.000332
Train Epoch: 1 [27000/28539 (95%)]	Loss: 6.709554	LR: 0.000343
Train Epoch: 1 [28000/28539 (98%)]	Loss: 6.682199	LR: 0.000354

evaluating...
Test set: Average loss: 6.6404, Average CER: 0.997403 Average WER: 0.9983

Fri May  7 17:00:58 UTC 2021
Train Epoch: 2 [0/28539 (0%)]	Loss: 6.593834	LR: 0.000360
Train Epoch: 2 [1000/28539 (4%)]	Loss: 6.647081	LR: 0.000371
Train Epoch: 2 [2000/28539 (7%)]	Loss: 6.610691	LR: 0.000383
Train Epoch: 2 [3000/28539 (11%)]	Loss: 6.317815	LR: 0.000394
Train Epoch: 2 [4000/28539 (14%)]	Loss: 6.242479	LR: 0.000405
Train Epoch: 2 [5000/28539 (18%)]	Loss: 6.305305	LR: 0.000416
Train Epoch: 2 [6000/28539 (21%)]	Loss: 6.432704	LR: 0.000427
Train Epoch: 2 [7000/28539 (25%)]	Loss: 6.359050	LR: 0.000439
Train Epoch: 2 [8000/28539 (28%)]	Loss: 6.456099	LR: 0.000450
Train Epoch: 2 [9000/28539 (32%)]	Loss: 5.605296	LR: 0.000461
Train Epoch: 2 [10000/28539 (35%)]	Loss: 6.250254	LR: 0.000472
Train Epoch: 2 [11000/28539 (39%)]	Loss: 6.062363	LR: 0.000483
Train Epoch: 2 [12000/28539 (42%)]	Loss: 6.074932	LR: 0.000495
Train Epoch: 2 [13000/28539 (46%)]	Loss: 6.041218	LR: 0.000506
Train Epoch: 2 [14000/28539 (49%)]	Loss: 5.776670	LR: 0.000517
Train Epoch: 2 [15000/28539 (53%)]	Loss: 5.883777	LR: 0.000528
Train Epoch: 2 [16000/28539 (56%)]	Loss: 5.573515	LR: 0.000540
Train Epoch: 2 [17000/28539 (60%)]	Loss: 5.522268	LR: 0.000551
Train Epoch: 2 [18000/28539 (63%)]	Loss: 5.819127	LR: 0.000562
Train Epoch: 2 [19000/28539 (67%)]	Loss: 5.515669	LR: 0.000573
Train Epoch: 2 [20000/28539 (70%)]	Loss: 5.699695	LR: 0.000584
Train Epoch: 2 [21000/28539 (74%)]	Loss: 5.611022	LR: 0.000596
Train Epoch: 2 [22000/28539 (77%)]	Loss: 5.305463	LR: 0.000607
Train Epoch: 2 [23000/28539 (81%)]	Loss: 5.215686	LR: 0.000618
Train Epoch: 2 [24000/28539 (84%)]	Loss: 5.183969	LR: 0.000629
Train Epoch: 2 [25000/28539 (88%)]	Loss: 4.886507	LR: 0.000640
Train Epoch: 2 [26000/28539 (91%)]	Loss: 5.434007	LR: 0.000652
Train Epoch: 2 [27000/28539 (95%)]	Loss: 5.380107	LR: 0.000663
Train Epoch: 2 [28000/28539 (98%)]	Loss: 5.143382	LR: 0.000674

evaluating...
Test set: Average loss: 4.8985, Average CER: 0.770797 Average WER: 0.8705

Fri May  7 17:30:46 UTC 2021
Train Epoch: 3 [0/28539 (0%)]	Loss: 5.107123	LR: 0.000680
Train Epoch: 3 [1000/28539 (4%)]	Loss: 5.379844	LR: 0.000691
Train Epoch: 3 [2000/28539 (7%)]	Loss: 5.013255	LR: 0.000703
Train Epoch: 3 [3000/28539 (11%)]	Loss: 5.366868	LR: 0.000714
Train Epoch: 3 [4000/28539 (14%)]	Loss: 5.266306	LR: 0.000725
Train Epoch: 3 [5000/28539 (18%)]	Loss: 4.956647	LR: 0.000736
Train Epoch: 3 [6000/28539 (21%)]	Loss: 5.118694	LR: 0.000747
Train Epoch: 3 [7000/28539 (25%)]	Loss: 4.630965	LR: 0.000759
Train Epoch: 3 [8000/28539 (28%)]	Loss: 5.041187	LR: 0.000770
Train Epoch: 3 [9000/28539 (32%)]	Loss: 5.080768	LR: 0.000781
Train Epoch: 3 [10000/28539 (35%)]	Loss: 4.930605	LR: 0.000792
Train Epoch: 3 [11000/28539 (39%)]	Loss: 5.170529	LR: 0.000804
Train Epoch: 3 [12000/28539 (42%)]	Loss: 4.857555	LR: 0.000815
Train Epoch: 3 [13000/28539 (46%)]	Loss: 4.766452	LR: 0.000826
Train Epoch: 3 [14000/28539 (49%)]	Loss: 4.900503	LR: 0.000837
Train Epoch: 3 [15000/28539 (53%)]	Loss: 5.139542	LR: 0.000848
Train Epoch: 3 [16000/28539 (56%)]	Loss: 5.053559	LR: 0.000860
Train Epoch: 3 [17000/28539 (60%)]	Loss: 4.403868	LR: 0.000871
Train Epoch: 3 [18000/28539 (63%)]	Loss: 4.828250	LR: 0.000882
Train Epoch: 3 [19000/28539 (67%)]	Loss: 4.934130	LR: 0.000893
Train Epoch: 3 [20000/28539 (70%)]	Loss: 4.768476	LR: 0.000904
Train Epoch: 3 [21000/28539 (74%)]	Loss: 4.631168	LR: 0.000916
Train Epoch: 3 [22000/28539 (77%)]	Loss: 4.705909	LR: 0.000927
Train Epoch: 3 [23000/28539 (81%)]	Loss: 4.737338	LR: 0.000938
Train Epoch: 3 [24000/28539 (84%)]	Loss: 4.786904	LR: 0.000949
Train Epoch: 3 [25000/28539 (88%)]	Loss: 4.721827	LR: 0.000961
Train Epoch: 3 [26000/28539 (91%)]	Loss: 4.593549	LR: 0.000972
Train Epoch: 3 [27000/28539 (95%)]	Loss: 4.969579	LR: 0.000983
Train Epoch: 3 [28000/28539 (98%)]	Loss: 4.730032	LR: 0.000994

evaluating...
Test set: Average loss: 4.3175, Average CER: 0.652336 Average WER: 0.7987

Fri May  7 18:01:38 UTC 2021
Train Epoch: 4 [0/28539 (0%)]	Loss: 4.573146	LR: 0.001000
Train Epoch: 4 [1000/28539 (4%)]	Loss: 4.856130	LR: 0.000995
Train Epoch: 4 [2000/28539 (7%)]	Loss: 4.557167	LR: 0.000990
Train Epoch: 4 [3000/28539 (11%)]	Loss: 4.660342	LR: 0.000985
Train Epoch: 4 [4000/28539 (14%)]	Loss: 4.585494	LR: 0.000980
Train Epoch: 4 [5000/28539 (18%)]	Loss: 4.597587	LR: 0.000975
Train Epoch: 4 [6000/28539 (21%)]	Loss: 4.549626	LR: 0.000970
Train Epoch: 4 [7000/28539 (25%)]	Loss: 4.782145	LR: 0.000965
Train Epoch: 4 [8000/28539 (28%)]	Loss: 4.088492	LR: 0.000960
Train Epoch: 4 [9000/28539 (32%)]	Loss: 4.809762	LR: 0.000955
Train Epoch: 4 [10000/28539 (35%)]	Loss: 4.710787	LR: 0.000950
Train Epoch: 4 [11000/28539 (39%)]	Loss: 4.737038	LR: 0.000945
Train Epoch: 4 [12000/28539 (42%)]	Loss: 4.492644	LR: 0.000940
Train Epoch: 4 [13000/28539 (46%)]	Loss: 4.496408	LR: 0.000935
Train Epoch: 4 [14000/28539 (49%)]	Loss: 4.920897	LR: 0.000930
Train Epoch: 4 [15000/28539 (53%)]	Loss: 4.774986	LR: 0.000925
Train Epoch: 4 [16000/28539 (56%)]	Loss: 4.462708	LR: 0.000920
Train Epoch: 4 [17000/28539 (60%)]	Loss: 4.260801	LR: 0.000915
Train Epoch: 4 [18000/28539 (63%)]	Loss: 3.938712	LR: 0.000910
Train Epoch: 4 [19000/28539 (67%)]	Loss: 4.614501	LR: 0.000905
Train Epoch: 4 [20000/28539 (70%)]	Loss: 4.387046	LR: 0.000900
Train Epoch: 4 [21000/28539 (74%)]	Loss: 4.518025	LR: 0.000895
Train Epoch: 4 [22000/28539 (77%)]	Loss: 4.325643	LR: 0.000890
Train Epoch: 4 [23000/28539 (81%)]	Loss: 4.089587	LR: 0.000885
Train Epoch: 4 [24000/28539 (84%)]	Loss: 4.071931	LR: 0.000880
Train Epoch: 4 [25000/28539 (88%)]	Loss: 4.489387	LR: 0.000875
Train Epoch: 4 [26000/28539 (91%)]	Loss: 4.314374	LR: 0.000870
Train Epoch: 4 [27000/28539 (95%)]	Loss: 4.127888	LR: 0.000865
Train Epoch: 4 [28000/28539 (98%)]	Loss: 4.062657	LR: 0.000860

evaluating...
Test set: Average loss: 3.6522, Average CER: 0.546940 Average WER: 0.7254

Fri May  7 18:33:22 UTC 2021
Train Epoch: 5 [0/28539 (0%)]	Loss: 3.885553	LR: 0.000857
Train Epoch: 5 [1000/28539 (4%)]	Loss: 4.219637	LR: 0.000852
Train Epoch: 5 [2000/28539 (7%)]	Loss: 3.900212	LR: 0.000847
Train Epoch: 5 [3000/28539 (11%)]	Loss: 3.896195	LR: 0.000842
Train Epoch: 5 [4000/28539 (14%)]	Loss: 3.841201	LR: 0.000837
Train Epoch: 5 [5000/28539 (18%)]	Loss: 3.655263	LR: 0.000832
Train Epoch: 5 [6000/28539 (21%)]	Loss: 3.895101	LR: 0.000827
Train Epoch: 5 [7000/28539 (25%)]	Loss: 3.670272	LR: 0.000822
Train Epoch: 5 [8000/28539 (28%)]	Loss: 3.839139	LR: 0.000817
Train Epoch: 5 [9000/28539 (32%)]	Loss: 3.926299	LR: 0.000812
Train Epoch: 5 [10000/28539 (35%)]	Loss: 4.023908	LR: 0.000807
Train Epoch: 5 [11000/28539 (39%)]	Loss: 3.711636	LR: 0.000802
Train Epoch: 5 [12000/28539 (42%)]	Loss: 3.382877	LR: 0.000797
Train Epoch: 5 [13000/28539 (46%)]	Loss: 3.957544	LR: 0.000792
Train Epoch: 5 [14000/28539 (49%)]	Loss: 3.684108	LR: 0.000787
Train Epoch: 5 [15000/28539 (53%)]	Loss: 3.780306	LR: 0.000782
Train Epoch: 5 [16000/28539 (56%)]	Loss: 3.424549	LR: 0.000777
Train Epoch: 5 [17000/28539 (60%)]	Loss: 3.491763	LR: 0.000772
Train Epoch: 5 [18000/28539 (63%)]	Loss: 3.538340	LR: 0.000767
Train Epoch: 5 [19000/28539 (67%)]	Loss: 4.062531	LR: 0.000762
Train Epoch: 5 [20000/28539 (70%)]	Loss: 3.498383	LR: 0.000757
Train Epoch: 5 [21000/28539 (74%)]	Loss: 4.088971	LR: 0.000752
Train Epoch: 5 [22000/28539 (77%)]	Loss: 3.599157	LR: 0.000747
Train Epoch: 5 [23000/28539 (81%)]	Loss: 3.622272	LR: 0.000742
Train Epoch: 5 [24000/28539 (84%)]	Loss: 3.352601	LR: 0.000737
Train Epoch: 5 [25000/28539 (88%)]	Loss: 3.638136	LR: 0.000732
Train Epoch: 5 [26000/28539 (91%)]	Loss: 3.225410	LR: 0.000727
Train Epoch: 5 [27000/28539 (95%)]	Loss: 3.550559	LR: 0.000722
Train Epoch: 5 [28000/28539 (98%)]	Loss: 3.267270	LR: 0.000717

evaluating...
Test set: Average loss: 3.0327, Average CER: 0.438167 Average WER: 0.6348

Fri May  7 19:06:05 UTC 2021
Train Epoch: 6 [0/28539 (0%)]	Loss: 3.126015	LR: 0.000714
Train Epoch: 6 [1000/28539 (4%)]	Loss: 3.243598	LR: 0.000709
Train Epoch: 6 [2000/28539 (7%)]	Loss: 3.429710	LR: 0.000704
Train Epoch: 6 [3000/28539 (11%)]	Loss: 3.593684	LR: 0.000699
Train Epoch: 6 [4000/28539 (14%)]	Loss: 3.403063	LR: 0.000694
Train Epoch: 6 [5000/28539 (18%)]	Loss: 3.467332	LR: 0.000689
Train Epoch: 6 [6000/28539 (21%)]	Loss: 3.352501	LR: 0.000684
Train Epoch: 6 [7000/28539 (25%)]	Loss: 3.521597	LR: 0.000679
Train Epoch: 6 [8000/28539 (28%)]	Loss: 3.251961	LR: 0.000674
Train Epoch: 6 [9000/28539 (32%)]	Loss: 3.253094	LR: 0.000669
Train Epoch: 6 [10000/28539 (35%)]	Loss: 3.556717	LR: 0.000664
Train Epoch: 6 [11000/28539 (39%)]	Loss: 3.256165	LR: 0.000659
Train Epoch: 6 [12000/28539 (42%)]	Loss: 3.589794	LR: 0.000654
Train Epoch: 6 [13000/28539 (46%)]	Loss: 3.291356	LR: 0.000649
Train Epoch: 6 [14000/28539 (49%)]	Loss: 3.748237	LR: 0.000644
Train Epoch: 6 [15000/28539 (53%)]	Loss: 3.332036	LR: 0.000639
Train Epoch: 6 [16000/28539 (56%)]	Loss: 3.434245	LR: 0.000634
Train Epoch: 6 [17000/28539 (60%)]	Loss: 3.943915	LR: 0.000629
Train Epoch: 6 [18000/28539 (63%)]	Loss: 3.041485	LR: 0.000624
Train Epoch: 6 [19000/28539 (67%)]	Loss: 3.223222	LR: 0.000619
Train Epoch: 6 [20000/28539 (70%)]	Loss: 3.675666	LR: 0.000614
Train Epoch: 6 [21000/28539 (74%)]	Loss: 3.157449	LR: 0.000609
Train Epoch: 6 [22000/28539 (77%)]	Loss: 2.992994	LR: 0.000604
Train Epoch: 6 [23000/28539 (81%)]	Loss: 3.576928	LR: 0.000599
Train Epoch: 6 [24000/28539 (84%)]	Loss: 3.093088	LR: 0.000594
Train Epoch: 6 [25000/28539 (88%)]	Loss: 3.051927	LR: 0.000589
Train Epoch: 6 [26000/28539 (91%)]	Loss: 3.291239	LR: 0.000584
Train Epoch: 6 [27000/28539 (95%)]	Loss: 3.247154	LR: 0.000579
Train Epoch: 6 [28000/28539 (98%)]	Loss: 3.068161	LR: 0.000574

evaluating...
Test set: Average loss: 2.6862, Average CER: 0.393001 Average WER: 0.5890

Fri May  7 19:39:00 UTC 2021
Train Epoch: 7 [0/28539 (0%)]	Loss: 3.312881	LR: 0.000571
Train Epoch: 7 [1000/28539 (4%)]	Loss: 2.808973	LR: 0.000566
Train Epoch: 7 [2000/28539 (7%)]	Loss: 3.082886	LR: 0.000561
Train Epoch: 7 [3000/28539 (11%)]	Loss: 3.183266	LR: 0.000556
Train Epoch: 7 [4000/28539 (14%)]	Loss: 2.991055	LR: 0.000551
Train Epoch: 7 [5000/28539 (18%)]	Loss: 2.959437	LR: 0.000546
Train Epoch: 7 [6000/28539 (21%)]	Loss: 2.975089	LR: 0.000541
Train Epoch: 7 [7000/28539 (25%)]	Loss: 2.890030	LR: 0.000536
Train Epoch: 7 [8000/28539 (28%)]	Loss: 3.238029	LR: 0.000531
Train Epoch: 7 [9000/28539 (32%)]	Loss: 3.173906	LR: 0.000526
Train Epoch: 7 [10000/28539 (35%)]	Loss: 2.659022	LR: 0.000521
Train Epoch: 7 [11000/28539 (39%)]	Loss: 3.061665	LR: 0.000516
Train Epoch: 7 [12000/28539 (42%)]	Loss: 3.270573	LR: 0.000511
Train Epoch: 7 [13000/28539 (46%)]	Loss: 2.880644	LR: 0.000506
Train Epoch: 7 [14000/28539 (49%)]	Loss: 2.963540	LR: 0.000501
Train Epoch: 7 [15000/28539 (53%)]	Loss: 3.272981	LR: 0.000496
Train Epoch: 7 [16000/28539 (56%)]	Loss: 2.832994	LR: 0.000491
Train Epoch: 7 [17000/28539 (60%)]	Loss: 2.786907	LR: 0.000486
Train Epoch: 7 [18000/28539 (63%)]	Loss: 2.724622	LR: 0.000481
Train Epoch: 7 [19000/28539 (67%)]	Loss: 3.445220	LR: 0.000476
Train Epoch: 7 [20000/28539 (70%)]	Loss: 2.949646	LR: 0.000471
Train Epoch: 7 [21000/28539 (74%)]	Loss: 2.691478	LR: 0.000466
Train Epoch: 7 [22000/28539 (77%)]	Loss: 3.223236	LR: 0.000461
Train Epoch: 7 [23000/28539 (81%)]	Loss: 2.568427	LR: 0.000456
Train Epoch: 7 [24000/28539 (84%)]	Loss: 3.030752	LR: 0.000451
Train Epoch: 7 [25000/28539 (88%)]	Loss: 2.783565	LR: 0.000446
Train Epoch: 7 [26000/28539 (91%)]	Loss: 3.172496	LR: 0.000441
Train Epoch: 7 [27000/28539 (95%)]	Loss: 2.941289	LR: 0.000436
Train Epoch: 7 [28000/28539 (98%)]	Loss: 2.766844	LR: 0.000431

evaluating...
Test set: Average loss: 2.4199, Average CER: 0.351861 Average WER: 0.5438

Fri May  7 20:11:36 UTC 2021
Train Epoch: 8 [0/28539 (0%)]	Loss: 2.856547	LR: 0.000428
Train Epoch: 8 [1000/28539 (4%)]	Loss: 2.749166	LR: 0.000423
Train Epoch: 8 [2000/28539 (7%)]	Loss: 3.296186	LR: 0.000418
Train Epoch: 8 [3000/28539 (11%)]	Loss: 2.791431	LR: 0.000413
Train Epoch: 8 [4000/28539 (14%)]	Loss: 2.660963	LR: 0.000408
Train Epoch: 8 [5000/28539 (18%)]	Loss: 2.740352	LR: 0.000403
Train Epoch: 8 [6000/28539 (21%)]	Loss: 2.636131	LR: 0.000398
Train Epoch: 8 [7000/28539 (25%)]	Loss: 2.926552	LR: 0.000393
Train Epoch: 8 [8000/28539 (28%)]	Loss: 2.826569	LR: 0.000388
Train Epoch: 8 [9000/28539 (32%)]	Loss: 2.658386	LR: 0.000383
Train Epoch: 8 [10000/28539 (35%)]	Loss: 2.597140	LR: 0.000378
Train Epoch: 8 [11000/28539 (39%)]	Loss: 2.922909	LR: 0.000373
Train Epoch: 8 [12000/28539 (42%)]	Loss: 2.306646	LR: 0.000368
Train Epoch: 8 [13000/28539 (46%)]	Loss: 3.007289	LR: 0.000363
Train Epoch: 8 [14000/28539 (49%)]	Loss: 2.761044	LR: 0.000358
Train Epoch: 8 [15000/28539 (53%)]	Loss: 2.712910	LR: 0.000353
Train Epoch: 8 [16000/28539 (56%)]	Loss: 2.989808	LR: 0.000348
Train Epoch: 8 [17000/28539 (60%)]	Loss: 2.677059	LR: 0.000343
Train Epoch: 8 [18000/28539 (63%)]	Loss: 2.834436	LR: 0.000338
Train Epoch: 8 [19000/28539 (67%)]	Loss: 2.616490	LR: 0.000333
Train Epoch: 8 [20000/28539 (70%)]	Loss: 2.814953	LR: 0.000328
Train Epoch: 8 [21000/28539 (74%)]	Loss: 2.478189	LR: 0.000323
Train Epoch: 8 [22000/28539 (77%)]	Loss: 2.678934	LR: 0.000318
Train Epoch: 8 [23000/28539 (81%)]	Loss: 2.584332	LR: 0.000313
Train Epoch: 8 [24000/28539 (84%)]	Loss: 2.685735	LR: 0.000308
Train Epoch: 8 [25000/28539 (88%)]	Loss: 2.796969	LR: 0.000303
Train Epoch: 8 [26000/28539 (91%)]	Loss: 2.720129	LR: 0.000298
Train Epoch: 8 [27000/28539 (95%)]	Loss: 2.615068	LR: 0.000293
Train Epoch: 8 [28000/28539 (98%)]	Loss: 3.035856	LR: 0.000288

evaluating...
Test set: Average loss: 2.2833, Average CER: 0.319371 Average WER: 0.5243

Fri May  7 20:44:32 UTC 2021
Train Epoch: 9 [0/28539 (0%)]	Loss: 2.415925	LR: 0.000286
Train Epoch: 9 [1000/28539 (4%)]	Loss: 2.798985	LR: 0.000281
Train Epoch: 9 [2000/28539 (7%)]	Loss: 2.727382	LR: 0.000276
Train Epoch: 9 [3000/28539 (11%)]	Loss: 2.485217	LR: 0.000271
Train Epoch: 9 [4000/28539 (14%)]	Loss: 2.509194	LR: 0.000266
Train Epoch: 9 [5000/28539 (18%)]	Loss: 2.824253	LR: 0.000261
Train Epoch: 9 [6000/28539 (21%)]	Loss: 2.349535	LR: 0.000256
Train Epoch: 9 [7000/28539 (25%)]	Loss: 2.688011	LR: 0.000251
Train Epoch: 9 [8000/28539 (28%)]	Loss: 2.510304	LR: 0.000246
Train Epoch: 9 [9000/28539 (32%)]	Loss: 2.246713	LR: 0.000241
Train Epoch: 9 [10000/28539 (35%)]	Loss: 2.486091	LR: 0.000236
Train Epoch: 9 [11000/28539 (39%)]	Loss: 2.388241	LR: 0.000231
Train Epoch: 9 [12000/28539 (42%)]	Loss: 2.760130	LR: 0.000226
Train Epoch: 9 [13000/28539 (46%)]	Loss: 2.744585	LR: 0.000221
Train Epoch: 9 [14000/28539 (49%)]	Loss: 2.499981	LR: 0.000216
Train Epoch: 9 [15000/28539 (53%)]	Loss: 2.335621	LR: 0.000211
Train Epoch: 9 [16000/28539 (56%)]	Loss: 2.826427	LR: 0.000206
Train Epoch: 9 [17000/28539 (60%)]	Loss: 3.019379	LR: 0.000201