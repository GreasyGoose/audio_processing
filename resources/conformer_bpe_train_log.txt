ConformerModel(
  (conv_in): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (3): ReLU()
  )
  (conv_out): Sequential(
    (0): Linear(in_features=640, out_features=360, bias=True)
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (encoder_layer): MultiSequential(
    (0): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (1): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (2): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (3): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (4): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (5): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (6): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (7): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (8): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
    (9): EncoderLayer(
      (self_attn): MultiHeadedAttention(
        (linear_q): Linear(in_features=360, out_features=360, bias=True)
        (linear_k): Linear(in_features=360, out_features=360, bias=True)
        (linear_v): Linear(in_features=360, out_features=360, bias=True)
        (linear_out): Linear(in_features=360, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (feed_forward): PositionwiseFeedForward(
        (w_1): Linear(in_features=360, out_features=1024, bias=True)
        (w_2): Linear(in_features=1024, out_features=360, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (activation): ReLU()
      )
      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (concat_linear): Sequential()
    )
  )
  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)
  (final_layer): Linear(in_features=360, out_features=4001, bias=True)
)
Num Model Parameters 14284849
Fri May  7 23:46:50 UTC 2021
Train Epoch: 1 [0/28539 (0%)]	Loss: 52.876720	LR: 0.000040
Train Epoch: 1 [1000/28539 (4%)]	Loss: 7.228498	LR: 0.000051
Train Epoch: 1 [2000/28539 (7%)]	Loss: 6.891717	LR: 0.000063
Train Epoch: 1 [3000/28539 (11%)]	Loss: 6.857337	LR: 0.000074
Train Epoch: 1 [4000/28539 (14%)]	Loss: 6.955213	LR: 0.000085
Train Epoch: 1 [5000/28539 (18%)]	Loss: 6.868241	LR: 0.000096
Train Epoch: 1 [6000/28539 (21%)]	Loss: 6.866759	LR: 0.000107
Train Epoch: 1 [7000/28539 (25%)]	Loss: 6.974304	LR: 0.000119
Train Epoch: 1 [8000/28539 (28%)]	Loss: 7.071786	LR: 0.000130
Train Epoch: 1 [9000/28539 (32%)]	Loss: 6.852877	LR: 0.000141
Train Epoch: 1 [10000/28539 (35%)]	Loss: 6.928719	LR: 0.000152
Train Epoch: 1 [11000/28539 (39%)]	Loss: 6.933452	LR: 0.000163
Train Epoch: 1 [12000/28539 (42%)]	Loss: 6.890958	LR: 0.000175
Train Epoch: 1 [13000/28539 (46%)]	Loss: 6.846207	LR: 0.000186
Train Epoch: 1 [14000/28539 (49%)]	Loss: 6.940254	LR: 0.000197
Train Epoch: 1 [15000/28539 (53%)]	Loss: 6.767045	LR: 0.000208
Train Epoch: 1 [16000/28539 (56%)]	Loss: 6.868340	LR: 0.000220
Train Epoch: 1 [17000/28539 (60%)]	Loss: 6.842098	LR: 0.000231
Train Epoch: 1 [18000/28539 (63%)]	Loss: 6.980134	LR: 0.000242
Train Epoch: 1 [19000/28539 (67%)]	Loss: 7.039634	LR: 0.000253
Train Epoch: 1 [20000/28539 (70%)]	Loss: 6.752397	LR: 0.000264
Train Epoch: 1 [21000/28539 (74%)]	Loss: 6.779884	LR: 0.000276
Train Epoch: 1 [22000/28539 (77%)]	Loss: 6.936831	LR: 0.000287
Train Epoch: 1 [23000/28539 (81%)]	Loss: 6.598039	LR: 0.000298
Train Epoch: 1 [24000/28539 (84%)]	Loss: 6.633672	LR: 0.000309
Train Epoch: 1 [25000/28539 (88%)]	Loss: 6.641898	LR: 0.000320
Train Epoch: 1 [26000/28539 (91%)]	Loss: 6.752507	LR: 0.000332
Train Epoch: 1 [27000/28539 (95%)]	Loss: 6.713057	LR: 0.000343
Train Epoch: 1 [28000/28539 (98%)]	Loss: 6.662388	LR: 0.000354

evaluating...
Test set: Average loss: 6.6328, Average CER: 0.997076 Average WER: 0.9979

Fri May  7 23:54:15 UTC 2021
Train Epoch: 2 [0/28539 (0%)]	Loss: 6.580247	LR: 0.000360
Train Epoch: 2 [1000/28539 (4%)]	Loss: 6.640965	LR: 0.000371
Train Epoch: 2 [2000/28539 (7%)]	Loss: 6.600604	LR: 0.000383
Train Epoch: 2 [3000/28539 (11%)]	Loss: 6.321660	LR: 0.000394
Train Epoch: 2 [4000/28539 (14%)]	Loss: 6.242759	LR: 0.000405
Train Epoch: 2 [5000/28539 (18%)]	Loss: 6.291473	LR: 0.000416
Train Epoch: 2 [6000/28539 (21%)]	Loss: 6.413478	LR: 0.000427
Train Epoch: 2 [7000/28539 (25%)]	Loss: 6.358007	LR: 0.000439
Train Epoch: 2 [8000/28539 (28%)]	Loss: 6.473280	LR: 0.000450
Train Epoch: 2 [9000/28539 (32%)]	Loss: 5.674850	LR: 0.000461
Train Epoch: 2 [10000/28539 (35%)]	Loss: 6.263775	LR: 0.000472
Train Epoch: 2 [11000/28539 (39%)]	Loss: 6.061572	LR: 0.000483
Train Epoch: 2 [12000/28539 (42%)]	Loss: 6.069897	LR: 0.000495
Train Epoch: 2 [13000/28539 (46%)]	Loss: 6.036027	LR: 0.000506
Train Epoch: 2 [14000/28539 (49%)]	Loss: 5.744599	LR: 0.000517
Train Epoch: 2 [15000/28539 (53%)]	Loss: 5.878890	LR: 0.000528
Train Epoch: 2 [16000/28539 (56%)]	Loss: 5.574967	LR: 0.000540
Train Epoch: 2 [17000/28539 (60%)]	Loss: 5.533855	LR: 0.000551
Train Epoch: 2 [18000/28539 (63%)]	Loss: 5.773177	LR: 0.000562
Train Epoch: 2 [19000/28539 (67%)]	Loss: 5.491566	LR: 0.000573
Train Epoch: 2 [20000/28539 (70%)]	Loss: 5.636652	LR: 0.000584
Train Epoch: 2 [21000/28539 (74%)]	Loss: 5.562602	LR: 0.000596
Train Epoch: 2 [22000/28539 (77%)]	Loss: 5.274246	LR: 0.000607
Train Epoch: 2 [23000/28539 (81%)]	Loss: 5.205345	LR: 0.000618
Train Epoch: 2 [24000/28539 (84%)]	Loss: 5.208617	LR: 0.000629
Train Epoch: 2 [25000/28539 (88%)]	Loss: 4.894951	LR: 0.000640
Train Epoch: 2 [26000/28539 (91%)]	Loss: 5.461395	LR: 0.000652
Train Epoch: 2 [27000/28539 (95%)]	Loss: 5.368482	LR: 0.000663
Train Epoch: 2 [28000/28539 (98%)]	Loss: 5.120448	LR: 0.000674

evaluating...
Test set: Average loss: 4.8862, Average CER: 0.764978 Average WER: 0.8687

Sat May  8 00:03:23 UTC 2021
Train Epoch: 3 [0/28539 (0%)]	Loss: 5.070038	LR: 0.000680
Train Epoch: 3 [1000/28539 (4%)]	Loss: 5.418457	LR: 0.000691
Train Epoch: 3 [2000/28539 (7%)]	Loss: 4.994422	LR: 0.000703
Train Epoch: 3 [3000/28539 (11%)]	Loss: 5.447978	LR: 0.000714
Train Epoch: 3 [4000/28539 (14%)]	Loss: 5.309944	LR: 0.000725
Train Epoch: 3 [5000/28539 (18%)]	Loss: 4.923997	LR: 0.000736
Train Epoch: 3 [6000/28539 (21%)]	Loss: 5.142769	LR: 0.000747
Train Epoch: 3 [7000/28539 (25%)]	Loss: 4.645734	LR: 0.000759
Train Epoch: 3 [8000/28539 (28%)]	Loss: 4.979085	LR: 0.000770
Train Epoch: 3 [9000/28539 (32%)]	Loss: 5.055873	LR: 0.000781
Train Epoch: 3 [10000/28539 (35%)]	Loss: 4.853648	LR: 0.000792
Train Epoch: 3 [11000/28539 (39%)]	Loss: 5.118386	LR: 0.000804
Train Epoch: 3 [12000/28539 (42%)]	Loss: 4.789643	LR: 0.000815
Train Epoch: 3 [13000/28539 (46%)]	Loss: 4.814443	LR: 0.000826
Train Epoch: 3 [14000/28539 (49%)]	Loss: 4.887559	LR: 0.000837
Train Epoch: 3 [15000/28539 (53%)]	Loss: 5.153343	LR: 0.000848
Train Epoch: 3 [16000/28539 (56%)]	Loss: 4.987537	LR: 0.000860
Train Epoch: 3 [17000/28539 (60%)]	Loss: 4.401402	LR: 0.000871
Train Epoch: 3 [18000/28539 (63%)]	Loss: 4.799230	LR: 0.000882
Train Epoch: 3 [19000/28539 (67%)]	Loss: 4.938310	LR: 0.000893
Train Epoch: 3 [20000/28539 (70%)]	Loss: 4.746801	LR: 0.000904
Train Epoch: 3 [21000/28539 (74%)]	Loss: 4.623869	LR: 0.000916
Train Epoch: 3 [22000/28539 (77%)]	Loss: 4.787691	LR: 0.000927
Train Epoch: 3 [23000/28539 (81%)]	Loss: 4.814672	LR: 0.000938
Train Epoch: 3 [24000/28539 (84%)]	Loss: 4.775430	LR: 0.000949
Train Epoch: 3 [25000/28539 (88%)]	Loss: 4.735715	LR: 0.000961
Train Epoch: 3 [26000/28539 (91%)]	Loss: 4.592324	LR: 0.000972
Train Epoch: 3 [27000/28539 (95%)]	Loss: 4.935316	LR: 0.000983
Train Epoch: 3 [28000/28539 (98%)]	Loss: 4.725890	LR: 0.000994

evaluating...
Test set: Average loss: 4.2889, Average CER: 0.641995 Average WER: 0.7987

Sat May  8 00:13:47 UTC 2021
Train Epoch: 4 [0/28539 (0%)]	Loss: 4.624618	LR: 0.001000
Train Epoch: 4 [1000/28539 (4%)]	Loss: 4.833268	LR: 0.000995
Train Epoch: 4 [2000/28539 (7%)]	Loss: 4.644696	LR: 0.000990
Train Epoch: 4 [3000/28539 (11%)]	Loss: 4.639291	LR: 0.000985
Train Epoch: 4 [4000/28539 (14%)]	Loss: 4.583451	LR: 0.000980
Train Epoch: 4 [5000/28539 (18%)]	Loss: 4.585273	LR: 0.000975
Train Epoch: 4 [6000/28539 (21%)]	Loss: 4.609611	LR: 0.000970
Train Epoch: 4 [7000/28539 (25%)]	Loss: 4.818323	LR: 0.000965
Train Epoch: 4 [8000/28539 (28%)]	Loss: 3.972031	LR: 0.000960
Train Epoch: 4 [9000/28539 (32%)]	Loss: 4.861034	LR: 0.000955
Train Epoch: 4 [10000/28539 (35%)]	Loss: 4.631765	LR: 0.000950
Train Epoch: 4 [11000/28539 (39%)]	Loss: 4.674203	LR: 0.000945
Train Epoch: 4 [12000/28539 (42%)]	Loss: 4.537690	LR: 0.000940
Train Epoch: 4 [13000/28539 (46%)]	Loss: 4.630840	LR: 0.000935
Train Epoch: 4 [14000/28539 (49%)]	Loss: 4.972964	LR: 0.000930
Train Epoch: 4 [15000/28539 (53%)]	Loss: 4.747749	LR: 0.000925
Train Epoch: 4 [16000/28539 (56%)]	Loss: 4.587430	LR: 0.000920
Train Epoch: 4 [17000/28539 (60%)]	Loss: 4.327804	LR: 0.000915
Train Epoch: 4 [18000/28539 (63%)]	Loss: 4.121680	LR: 0.000910
Train Epoch: 4 [19000/28539 (67%)]	Loss: 4.702386	LR: 0.000905
Train Epoch: 4 [20000/28539 (70%)]	Loss: 4.435782	LR: 0.000900
Train Epoch: 4 [21000/28539 (74%)]	Loss: 4.499433	LR: 0.000895
Train Epoch: 4 [22000/28539 (77%)]	Loss: 4.379106	LR: 0.000890
Train Epoch: 4 [23000/28539 (81%)]	Loss: 4.183766	LR: 0.000885
Train Epoch: 4 [24000/28539 (84%)]	Loss: 4.125965	LR: 0.000880
Train Epoch: 4 [25000/28539 (88%)]	Loss: 4.535967	LR: 0.000875
Train Epoch: 4 [26000/28539 (91%)]	Loss: 4.441625	LR: 0.000870
Train Epoch: 4 [27000/28539 (95%)]	Loss: 4.102516	LR: 0.000865
Train Epoch: 4 [28000/28539 (98%)]	Loss: 4.089907	LR: 0.000860

evaluating...
Test set: Average loss: 3.6707, Average CER: 0.549232 Average WER: 0.7262

Sat May  8 00:24:55 UTC 2021
Train Epoch: 5 [0/28539 (0%)]	Loss: 3.902666	LR: 0.000857
Train Epoch: 5 [1000/28539 (4%)]	Loss: 4.189885	LR: 0.000852
Train Epoch: 5 [2000/28539 (7%)]	Loss: 3.946101	LR: 0.000847
Train Epoch: 5 [3000/28539 (11%)]	Loss: 3.822058	LR: 0.000842
Train Epoch: 5 [4000/28539 (14%)]	Loss: 3.754586	LR: 0.000837
Train Epoch: 5 [5000/28539 (18%)]	Loss: 3.740934	LR: 0.000832
Train Epoch: 5 [6000/28539 (21%)]	Loss: 3.863225	LR: 0.000827
Train Epoch: 5 [7000/28539 (25%)]	Loss: 3.773616	LR: 0.000822
Train Epoch: 5 [8000/28539 (28%)]	Loss: 3.867498	LR: 0.000817
Train Epoch: 5 [9000/28539 (32%)]	Loss: 3.925074	LR: 0.000812
Train Epoch: 5 [10000/28539 (35%)]	Loss: 4.073742	LR: 0.000807
Train Epoch: 5 [11000/28539 (39%)]	Loss: 3.747732	LR: 0.000802
Train Epoch: 5 [12000/28539 (42%)]	Loss: 3.419233	LR: 0.000797
Train Epoch: 5 [13000/28539 (46%)]	Loss: 3.842652	LR: 0.000792
Train Epoch: 5 [14000/28539 (49%)]	Loss: 3.651168	LR: 0.000787
Train Epoch: 5 [15000/28539 (53%)]	Loss: 3.691930	LR: 0.000782
Train Epoch: 5 [16000/28539 (56%)]	Loss: 3.390234	LR: 0.000777
Train Epoch: 5 [17000/28539 (60%)]	Loss: 3.431282	LR: 0.000772
Train Epoch: 5 [18000/28539 (63%)]	Loss: 3.403530	LR: 0.000767
Train Epoch: 5 [19000/28539 (67%)]	Loss: 4.076526	LR: 0.000762
Train Epoch: 5 [20000/28539 (70%)]	Loss: 3.423553	LR: 0.000757
Train Epoch: 5 [21000/28539 (74%)]	Loss: 4.023041	LR: 0.000752
Train Epoch: 5 [22000/28539 (77%)]	Loss: 3.589930	LR: 0.000747
Train Epoch: 5 [23000/28539 (81%)]	Loss: 3.484160	LR: 0.000742
Train Epoch: 5 [24000/28539 (84%)]	Loss: 3.188579	LR: 0.000737
Train Epoch: 5 [25000/28539 (88%)]	Loss: 3.551989	LR: 0.000732
Train Epoch: 5 [26000/28539 (91%)]	Loss: 3.137432	LR: 0.000727
Train Epoch: 5 [27000/28539 (95%)]	Loss: 3.488846	LR: 0.000722
Train Epoch: 5 [28000/28539 (98%)]	Loss: 3.187975	LR: 0.000717

evaluating...
Test set: Average loss: 2.9451, Average CER: 0.421262 Average WER: 0.6161

Sat May  8 00:36:53 UTC 2021
Train Epoch: 6 [0/28539 (0%)]	Loss: 3.030920	LR: 0.000714
Train Epoch: 6 [1000/28539 (4%)]	Loss: 3.241415	LR: 0.000709
Train Epoch: 6 [2000/28539 (7%)]	Loss: 3.314355	LR: 0.000704
Train Epoch: 6 [3000/28539 (11%)]	Loss: 3.604044	LR: 0.000699
Train Epoch: 6 [4000/28539 (14%)]	Loss: 3.338143	LR: 0.000694
Train Epoch: 6 [5000/28539 (18%)]	Loss: 3.365569	LR: 0.000689
Train Epoch: 6 [6000/28539 (21%)]	Loss: 3.273225	LR: 0.000684
Train Epoch: 6 [7000/28539 (25%)]	Loss: 3.354326	LR: 0.000679
Train Epoch: 6 [8000/28539 (28%)]	Loss: 3.174377	LR: 0.000674
Train Epoch: 6 [9000/28539 (32%)]	Loss: 3.196548	LR: 0.000669
Train Epoch: 6 [10000/28539 (35%)]	Loss: 3.479354	LR: 0.000664
Train Epoch: 6 [11000/28539 (39%)]	Loss: 3.260267	LR: 0.000659
Train Epoch: 6 [12000/28539 (42%)]	Loss: 3.374526	LR: 0.000654
Train Epoch: 6 [13000/28539 (46%)]	Loss: 3.237626	LR: 0.000649
Train Epoch: 6 [14000/28539 (49%)]	Loss: 3.680523	LR: 0.000644
Train Epoch: 6 [15000/28539 (53%)]	Loss: 3.331900	LR: 0.000639
Train Epoch: 6 [16000/28539 (56%)]	Loss: 3.337759	LR: 0.000634
Train Epoch: 6 [17000/28539 (60%)]	Loss: 3.898309	LR: 0.000629
Train Epoch: 6 [18000/28539 (63%)]	Loss: 2.967719	LR: 0.000624
Train Epoch: 6 [19000/28539 (67%)]	Loss: 3.125717	LR: 0.000619
Train Epoch: 6 [20000/28539 (70%)]	Loss: 3.566581	LR: 0.000614
Train Epoch: 6 [21000/28539 (74%)]	Loss: 3.044081	LR: 0.000609
Train Epoch: 6 [22000/28539 (77%)]	Loss: 2.984319	LR: 0.000604
Train Epoch: 6 [23000/28539 (81%)]	Loss: 3.560327	LR: 0.000599
Train Epoch: 6 [24000/28539 (84%)]	Loss: 3.053452	LR: 0.000594
Train Epoch: 6 [25000/28539 (88%)]	Loss: 2.973967	LR: 0.000589
Train Epoch: 6 [26000/28539 (91%)]	Loss: 3.114827	LR: 0.000584
Train Epoch: 6 [27000/28539 (95%)]	Loss: 3.217822	LR: 0.000579
Train Epoch: 6 [28000/28539 (98%)]	Loss: 3.064291	LR: 0.000574

evaluating...
Test set: Average loss: 2.6035, Average CER: 0.383968 Average WER: 0.5724

Sat May  8 00:48:51 UTC 2021
Train Epoch: 7 [0/28539 (0%)]	Loss: 3.175291	LR: 0.000571
Train Epoch: 7 [1000/28539 (4%)]	Loss: 2.719580	LR: 0.000566
Train Epoch: 7 [2000/28539 (7%)]	Loss: 3.020335	LR: 0.000561
Train Epoch: 7 [3000/28539 (11%)]	Loss: 2.983001	LR: 0.000556
Train Epoch: 7 [4000/28539 (14%)]	Loss: 2.909388	LR: 0.000551
Train Epoch: 7 [5000/28539 (18%)]	Loss: 2.937099	LR: 0.000546
Train Epoch: 7 [6000/28539 (21%)]	Loss: 3.073853	LR: 0.000541
Train Epoch: 7 [7000/28539 (25%)]	Loss: 2.797084	LR: 0.000536
Train Epoch: 7 [8000/28539 (28%)]	Loss: 3.245282	LR: 0.000531
Train Epoch: 7 [9000/28539 (32%)]	Loss: 3.070700	LR: 0.000526
Train Epoch: 7 [10000/28539 (35%)]	Loss: 2.637846	LR: 0.000521
Train Epoch: 7 [11000/28539 (39%)]	Loss: 2.948393	LR: 0.000516
Train Epoch: 7 [12000/28539 (42%)]	Loss: 3.151863	LR: 0.000511
Train Epoch: 7 [13000/28539 (46%)]	Loss: 2.785854	LR: 0.000506
Train Epoch: 7 [14000/28539 (49%)]	Loss: 2.891026	LR: 0.000501
Train Epoch: 7 [15000/28539 (53%)]	Loss: 3.192199	LR: 0.000496
Train Epoch: 7 [16000/28539 (56%)]	Loss: 2.769509	LR: 0.000491
Train Epoch: 7 [17000/28539 (60%)]	Loss: 2.761812	LR: 0.000486
Train Epoch: 7 [18000/28539 (63%)]	Loss: 2.703018	LR: 0.000481
Train Epoch: 7 [19000/28539 (67%)]	Loss: 3.266354	LR: 0.000476
Train Epoch: 7 [20000/28539 (70%)]	Loss: 3.013171	LR: 0.000471
Train Epoch: 7 [21000/28539 (74%)]	Loss: 2.693823	LR: 0.000466
Train Epoch: 7 [22000/28539 (77%)]	Loss: 3.121644	LR: 0.000461
Train Epoch: 7 [23000/28539 (81%)]	Loss: 2.554303	LR: 0.000456
Train Epoch: 7 [24000/28539 (84%)]	Loss: 3.086833	LR: 0.000451
Train Epoch: 7 [25000/28539 (88%)]	Loss: 2.674268	LR: 0.000446
Train Epoch: 7 [26000/28539 (91%)]	Loss: 3.130710	LR: 0.000441
Train Epoch: 7 [27000/28539 (95%)]	Loss: 2.926009	LR: 0.000436
Train Epoch: 7 [28000/28539 (98%)]	Loss: 2.786557	LR: 0.000431

evaluating...
Test set: Average loss: 2.3664, Average CER: 0.337513 Average WER: 0.5319

Sat May  8 01:00:56 UTC 2021
Train Epoch: 8 [0/28539 (0%)]	Loss: 2.777666	LR: 0.000428
Train Epoch: 8 [1000/28539 (4%)]	Loss: 2.663428	LR: 0.000423
Train Epoch: 8 [2000/28539 (7%)]	Loss: 3.128880	LR: 0.000418
Train Epoch: 8 [3000/28539 (11%)]	Loss: 2.763853	LR: 0.000413
Train Epoch: 8 [4000/28539 (14%)]	Loss: 2.560644	LR: 0.000408
Train Epoch: 8 [5000/28539 (18%)]	Loss: 2.742617	LR: 0.000403
Train Epoch: 8 [6000/28539 (21%)]	Loss: 2.584247	LR: 0.000398
Train Epoch: 8 [7000/28539 (25%)]	Loss: 2.876015	LR: 0.000393
Train Epoch: 8 [8000/28539 (28%)]	Loss: 2.753725	LR: 0.000388
Train Epoch: 8 [9000/28539 (32%)]	Loss: 2.516611	LR: 0.000383
Train Epoch: 8 [10000/28539 (35%)]	Loss: 2.691837	LR: 0.000378
Train Epoch: 8 [11000/28539 (39%)]	Loss: 2.900200	LR: 0.000373
Train Epoch: 8 [12000/28539 (42%)]	Loss: 2.168820	LR: 0.000368
Train Epoch: 8 [13000/28539 (46%)]	Loss: 2.960646	LR: 0.000363
Train Epoch: 8 [14000/28539 (49%)]	Loss: 2.759516	LR: 0.000358
Train Epoch: 8 [15000/28539 (53%)]	Loss: 2.730174	LR: 0.000353
Train Epoch: 8 [16000/28539 (56%)]	Loss: 2.822701	LR: 0.000348
Train Epoch: 8 [17000/28539 (60%)]	Loss: 2.734856	LR: 0.000343
Train Epoch: 8 [18000/28539 (63%)]	Loss: 2.761109	LR: 0.000338
Train Epoch: 8 [19000/28539 (67%)]	Loss: 2.576290	LR: 0.000333
Train Epoch: 8 [20000/28539 (70%)]	Loss: 2.713374	LR: 0.000328
Train Epoch: 8 [21000/28539 (74%)]	Loss: 2.412234	LR: 0.000323
Train Epoch: 8 [22000/28539 (77%)]	Loss: 2.566166	LR: 0.000318
Train Epoch: 8 [23000/28539 (81%)]	Loss: 2.525761	LR: 0.000313
Train Epoch: 8 [24000/28539 (84%)]	Loss: 2.632055	LR: 0.000308
Train Epoch: 8 [25000/28539 (88%)]	Loss: 2.755566	LR: 0.000303
Train Epoch: 8 [26000/28539 (91%)]	Loss: 2.629764	LR: 0.000298
Train Epoch: 8 [27000/28539 (95%)]	Loss: 2.609128	LR: 0.000293
Train Epoch: 8 [28000/28539 (98%)]	Loss: 2.889865	LR: 0.000288

evaluating...
Test set: Average loss: 2.2077, Average CER: 0.308097 Average WER: 0.5090

Sat May  8 01:13:18 UTC 2021
Train Epoch: 9 [0/28539 (0%)]	Loss: 2.530916	LR: 0.000286
Train Epoch: 9 [1000/28539 (4%)]	Loss: 2.698689	LR: 0.000281
Train Epoch: 9 [2000/28539 (7%)]	Loss: 2.566475	LR: 0.000276
Train Epoch: 9 [3000/28539 (11%)]	Loss: 2.498351	LR: 0.000271
Train Epoch: 9 [4000/28539 (14%)]	Loss: 2.384259	LR: 0.000266
Train Epoch: 9 [5000/28539 (18%)]	Loss: 2.744260	LR: 0.000261
Train Epoch: 9 [6000/28539 (21%)]	Loss: 2.416678	LR: 0.000256
Train Epoch: 9 [7000/28539 (25%)]	Loss: 2.699442	LR: 0.000251
Train Epoch: 9 [8000/28539 (28%)]	Loss: 2.478687	LR: 0.000246
Train Epoch: 9 [9000/28539 (32%)]	Loss: 2.230560	LR: 0.000241
Train Epoch: 9 [10000/28539 (35%)]	Loss: 2.353805	LR: 0.000236
Train Epoch: 9 [11000/28539 (39%)]	Loss: 2.276982	LR: 0.000231
Train Epoch: 9 [12000/28539 (42%)]	Loss: 2.663586	LR: 0.000226
Train Epoch: 9 [13000/28539 (46%)]	Loss: 2.729818	LR: 0.000221
Train Epoch: 9 [14000/28539 (49%)]	Loss: 2.345521	LR: 0.000216
Train Epoch: 9 [15000/28539 (53%)]	Loss: 2.279125	LR: 0.000211
Train Epoch: 9 [16000/28539 (56%)]	Loss: 2.693343	LR: 0.000206
Train Epoch: 9 [17000/28539 (60%)]	Loss: 2.981220	LR: 0.000201
Train Epoch: 9 [18000/28539 (63%)]	Loss: 2.608375	LR: 0.000196
Train Epoch: 9 [19000/28539 (67%)]	Loss: 2.796263	LR: 0.000191
Train Epoch: 9 [20000/28539 (70%)]	Loss: 2.399392	LR: 0.000186
Train Epoch: 9 [21000/28539 (74%)]	Loss: 2.577857	LR: 0.000181
Train Epoch: 9 [22000/28539 (77%)]	Loss: 2.565780	LR: 0.000175
Train Epoch: 9 [23000/28539 (81%)]	Loss: 2.224485	LR: 0.000170
Train Epoch: 9 [24000/28539 (84%)]	Loss: 2.858747	LR: 0.000165
Train Epoch: 9 [25000/28539 (88%)]	Loss: 2.607889	LR: 0.000160
Train Epoch: 9 [26000/28539 (91%)]	Loss: 2.643822	LR: 0.000155
Train Epoch: 9 [27000/28539 (95%)]	Loss: 2.813597	LR: 0.000150
Train Epoch: 9 [28000/28539 (98%)]	Loss: 2.377969	LR: 0.000145

evaluating...
Test set: Average loss: 2.0867, Average CER: 0.298874 Average WER: 0.4881

Sat May  8 01:25:39 UTC 2021
Train Epoch: 10 [0/28539 (0%)]	Loss: 2.298668	LR: 0.000143
Train Epoch: 10 [1000/28539 (4%)]	Loss: 2.488280	LR: 0.000138
Train Epoch: 10 [2000/28539 (7%)]	Loss: 2.402967	LR: 0.000133
Train Epoch: 10 [3000/28539 (11%)]	Loss: 2.376529	LR: 0.000128
Train Epoch: 10 [4000/28539 (14%)]	Loss: 2.055771	LR: 0.000123
Train Epoch: 10 [5000/28539 (18%)]	Loss: 2.326540	LR: 0.000118
Train Epoch: 10 [6000/28539 (21%)]	Loss: 2.418869	LR: 0.000113
Train Epoch: 10 [7000/28539 (25%)]	Loss: 2.254887	LR: 0.000108
Train Epoch: 10 [8000/28539 (28%)]	Loss: 2.144580	LR: 0.000103
Train Epoch: 10 [9000/28539 (32%)]	Loss: 2.370577	LR: 0.000098
Train Epoch: 10 [10000/28539 (35%)]	Loss: 2.595356	LR: 0.000093
Train Epoch: 10 [11000/28539 (39%)]	Loss: 2.327622	LR: 0.000088
Train Epoch: 10 [12000/28539 (42%)]	Loss: 2.197954	LR: 0.000083
Train Epoch: 10 [13000/28539 (46%)]	Loss: 2.352547	LR: 0.000078
Train Epoch: 10 [14000/28539 (49%)]	Loss: 2.430652	LR: 0.000073
Train Epoch: 10 [15000/28539 (53%)]	Loss: 2.405365	LR: 0.000068
Train Epoch: 10 [16000/28539 (56%)]	Loss: 2.519240	LR: 0.000063
Train Epoch: 10 [17000/28539 (60%)]	Loss: 2.426099	LR: 0.000058
Train Epoch: 10 [18000/28539 (63%)]	Loss: 2.281765	LR: 0.000053
Train Epoch: 10 [19000/28539 (67%)]	Loss: 2.576741	LR: 0.000048
Train Epoch: 10 [20000/28539 (70%)]	Loss: 2.314543	LR: 0.000043
Train Epoch: 10 [21000/28539 (74%)]	Loss: 2.183733	LR: 0.000038
Train Epoch: 10 [22000/28539 (77%)]	Loss: 2.422426	LR: 0.000033
Train Epoch: 10 [23000/28539 (81%)]	Loss: 2.478443	LR: 0.000028
Train Epoch: 10 [24000/28539 (84%)]	Loss: 2.462454	LR: 0.000023
Train Epoch: 10 [25000/28539 (88%)]	Loss: 2.602679	LR: 0.000018
Train Epoch: 10 [26000/28539 (91%)]	Loss: 2.373208	LR: 0.000013
Train Epoch: 10 [27000/28539 (95%)]	Loss: 1.816917	LR: 0.000008
Train Epoch: 10 [28000/28539 (98%)]	Loss: 2.030587	LR: 0.000003

evaluating...
Test set: Average loss: 2.0235, Average CER: 0.287857 Average WER: 0.4776